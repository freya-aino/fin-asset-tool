{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tropical-month",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `haskey(::TargetIterator, name::String)` is deprecated, use `Target(; name=name) !== nothing` instead.\n",
      "│   caller = llvm_support(::VersionNumber) at CUDAnative.jl:158\n",
      "└ @ CUDAnative C:\\Users\\noone\\.julia\\packages\\CUDAnative\\Phjco\\src\\CUDAnative.jl:158\n",
      "┌ Warning: CuArrays.jl found cuda, but did not find libcudnn. Some functionality will not be available.\n",
      "└ @ Flux C:\\Users\\noone\\.julia\\packages\\Flux\\NpkMm\\src\\Flux.jl:48\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using Flux\n",
    "using StatsBase\n",
    "# using Plots\n",
    "# using PyCall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9126e0",
   "metadata": {},
   "source": [
    "### ROADMAP\n",
    "\n",
    "\n",
    "1. make forward pass possible and trainable\n",
    "2. train a model to encode a sentence (reconstruction)\n",
    "3. train a model to predict one sentence from another\n",
    "4. construct: extract prediction labels from day\n",
    "5. prepaer (x, y) pairs \n",
    "6. tran a model to predict (x, y) pairs\n",
    "\n",
    "> - each set of samples: [s1, s2, ...] ~ X where X = [[x11, x12, ...], [x21, x22, ...], ...]\n",
    "\n",
    "> - should give varying activation strengths on a combination of sequential pivots and their time deltas = [y_s1, y_s2, ...]\n",
    "\n",
    "> - these activation pattenrs are then:\n",
    "\n",
    "> - 1. evaluated against the real pivots (time_delta, high/low, magnitude)\n",
    "\n",
    "> - 2. fed back into the network\n",
    "\n",
    "> - 3. save metrics about the goodness of the prediction in the network (e.g. sum(time_delta) = time_market_open, high/low correct, RMSE(magnitude - *))\n",
    "\n",
    "> after optimizing for a number of days, the model is saved and a set of new ones is created if it outperformed its peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d8fea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activate (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Activation\n",
    "    position::AbstractArray{Real}\n",
    "    energy::Real\n",
    "end\n",
    "\n",
    "struct Node\n",
    "    position::AbstractArray{Real}\n",
    "end\n",
    "\n",
    "struct Neuron\n",
    "    dendrites::AbstractArray{Node}\n",
    "    key::AbstractArray{Real}\n",
    "    axon_terminals::AbstractArray{Node}\n",
    "end\n",
    "\n",
    "mean = (x)->sum(x)/length(x)\n",
    "rbfk(x1::AbstractArray{Real, 1}, x2::AbstractArray{Real, 1}, σ::Real) = ℯ ^ -((mean((x1 .- x2) .^ 2) / σ))\n",
    "\n",
    "activate(neuron::Neuron, activations::Activation; min_energy = 0.01) = begin\n",
    "    \n",
    "    #TODO: filter relevant activations to reduce compute time\n",
    "\n",
    "    dendrite_value = [mean([((a.energy + rbfk(d.position, a.position, a.energy * d.sensitivity + min_energy)) / 2) for a in activations]) for d in neuron.dendrites]\n",
    "\n",
    "    #TODO: remove zeros fields gradients from backprop graph\n",
    "\n",
    "    out_energy = rbfk(neuron.key, dendrite_value)\n",
    "\n",
    "    return [Activation(a.position, out_energy) for a in neuron.axon_terminals]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60e4443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutable struct ParameterizedNormal # for reparameterization trick\n",
    "#     μ::Real\n",
    "#     σ::Real\n",
    "# end\n",
    "\n",
    "# struct ProtoNode\n",
    "#     position::AbstractArray{ParameterizedNormal}\n",
    "# end\n",
    "\n",
    "# struct ProtoNeuron\n",
    "#     key::ProtoNode # M-d\n",
    "#     proto_dendrites::AbstractArray{ProtoNode} # UxN-d\n",
    "#     proto_axons::AbstractArray{ProtoNode} # OxN-d\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b8ac15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProtoNeuron(ProtoNode(ParameterizedNormal(-1.02423, 0.752372)), ProtoNode(ParameterizedNormal(0.888649, 1.01902)), ProtoNode(ParameterizedNormal(0.341683, -0.118217)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# proto_proto_neuron = ProtoNeuron(\n",
    "#     ProtoNode([ParameterizedNormal(randn(), randn()) for _ in params[\"proto_proto_number_of_dendrites\"]]),\n",
    "#     [ProtoNode([ParameterizedNormal(randn(), randn()) for _ in params[\"network_space_dims\"]]) for _ in params[\"proto_proto_number_of_dendrites\"]],\n",
    "#     [ProtoNode([ParameterizedNormal(randn(), randn()) for _ in params[\"network_space_dims\"]]) for _ in params[\"proto_proto_number_of_axon_terminals\"]]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample(proto_neuron) = begin\n",
    "#     dendrites =  []\n",
    "#     Neuron()\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eba45b24",
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "\u001b[91mMethodError: no method matching randn(::Int64; mean=0, std=1)\u001b[39m\n\u001b[91m\u001b[0mClosest candidates are:\u001b[39m\n\u001b[91m\u001b[0m  randn(::Integer...) at C:\\cygwin\\home\\Administrator\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.0\\Random\\src\\normal.jl:188\u001b[91m got unsupported keyword arguments \"mean\", \"std\"\u001b[39m\u001b[39m\n\u001b[91m\u001b[0m  randn(\u001b[91m::CuArrays.CURAND.RNG\u001b[39m, \u001b[91m::Union{Type{Float32}, Type{Float64}}\u001b[39m, \u001b[91m::Tuple{Vararg{Int64,N}} where N\u001b[39m; kwargs...) at C:\\Users\\noone\\.julia\\packages\\CuArrays\\rNxse\\src\\rand\\random.jl:135\u001b[39m\n\u001b[91m\u001b[0m  randn(\u001b[91m::CuArrays.CURAND.RNG\u001b[39m, \u001b[91m::Tuple{Vararg{Int64,N}} where N\u001b[39m; kwargs...) at C:\\Users\\noone\\.julia\\packages\\CuArrays\\rNxse\\src\\rand\\random.jl:144\u001b[39m\n\u001b[91m\u001b[0m  ...\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: no method matching randn(::Int64; mean=0, std=1)\u001b[39m\n\u001b[91m\u001b[0mClosest candidates are:\u001b[39m\n\u001b[91m\u001b[0m  randn(::Integer...) at C:\\cygwin\\home\\Administrator\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.0\\Random\\src\\normal.jl:188\u001b[91m got unsupported keyword arguments \"mean\", \"std\"\u001b[39m\u001b[39m\n\u001b[91m\u001b[0m  randn(\u001b[91m::CuArrays.CURAND.RNG\u001b[39m, \u001b[91m::Union{Type{Float32}, Type{Float64}}\u001b[39m, \u001b[91m::Tuple{Vararg{Int64,N}} where N\u001b[39m; kwargs...) at C:\\Users\\noone\\.julia\\packages\\CuArrays\\rNxse\\src\\rand\\random.jl:135\u001b[39m\n\u001b[91m\u001b[0m  randn(\u001b[91m::CuArrays.CURAND.RNG\u001b[39m, \u001b[91m::Tuple{Vararg{Int64,N}} where N\u001b[39m; kwargs...) at C:\\Users\\noone\\.julia\\packages\\CuArrays\\rNxse\\src\\rand\\random.jl:144\u001b[39m\n\u001b[91m\u001b[0m  ...\u001b[39m",
      "",
      "Stacktrace:",
      " [1] (::getfield(Main, Symbol(\"##21#23\")))(::Int64) at .\\none:0",
      " [2] iterate at .\\generator.jl:47 [inlined]",
      " [3] collect(::Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##21#23\"))}) at .\\array.jl:619",
      " [4] top-level scope at In[33]:19"
     ]
    }
   ],
   "source": [
    "struct Activation\n",
    "    position::AbstractArray{Real}\n",
    "    energy::Real\n",
    "end\n",
    "\n",
    "struct DiscreteActivationalSet\n",
    "    input_positions::AbstractArray{AbstractArray{Real}}\n",
    "    output_positions::AbstractArray{AbstractArray{Real}}\n",
    "    key::AbstractArray{Real}\n",
    "\n",
    "    sensitvity::Real\n",
    "end\n",
    "\n",
    "\n",
    "network_dims = 2\n",
    "num_input_positions = 3\n",
    "num_output_positions = 3\n",
    "min_energy = 1\n",
    "\n",
    "\n",
    "discrete_activational_sets = [\n",
    "    DiscreteActivationalSet(\n",
    "        [randn(Normal(0, 1)) for _ in 1:num_input_positions],\n",
    "        [randn(network_dims, mean = 0, std = 1) for _ in 1:num_output_positions],\n",
    "        randn(num_input_positions, mean = 0, std = 1),\n",
    "        1.)\n",
    "]\n",
    "\n",
    "mean = (x)->sum(x)/length(x)\n",
    "rbfk(x1::AbstractArray{Real, 1}, x2::AbstractArray{Real, 1}, σ::Real) = ℯ ^ -((mean((x1 .- x2) .^ 2) / σ))\n",
    "activations = []\n",
    "\n",
    "for DAS in discrete_activational_sets\n",
    "    \n",
    "    for a in activations\n",
    "        for in_pos in DAS.input_positions\n",
    "            mean_act = [rbfk(in_pos, a.position, a.energy * .sensitivity + min_energy) \n",
    "        out_energy = rbfk(neuron.key, dendrite_value)\n",
    "\n",
    "    return [Activation(a.position, out_energy) for a in neuron.axon_terminals]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "global_model_prior = rand(global_model_prior_size)\n",
    "global_data_prior = rand(global_data_prior_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352b5295",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91msyntax: incomplete: premature end of input\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91msyntax: incomplete: premature end of input\u001b[39m",
      ""
     ]
    }
   ],
   "source": [
    "function train()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1b6aed",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "\u001b[91mUndefVarError: Chain not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: Chain not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] top-level scope at none:0"
     ]
    }
   ],
   "source": [
    "# mutable struct ParameterizedNormal # for reparameterization trick\n",
    "#     μ::Real\n",
    "#     σ::Real\n",
    "# end\n",
    "\n",
    "# struct ProtoNode\n",
    "#     position::AbstractArray{ParameterizedNormal}\n",
    "# end\n",
    "\n",
    "# struct ProtoNeuron\n",
    "#     key::ProtoNode # M-d\n",
    "#     proto_dendrites::AbstractArray{ProtoNode} # UxN-d\n",
    "#     proto_axons::AbstractArray{ProtoNode} # OxN-d\n",
    "# end\n",
    "\n",
    "\n",
    "struct NeuronTypeGenerator\n",
    "    number_of_dendrites::UnitRange\n",
    "    number_of_axons::UnitRange\n",
    "    \n",
    "    dendrite_generator_model::Chain\n",
    "    axon_generator_model::Chain\n",
    "    key_model::Chain\n",
    "end\n",
    "\n",
    "struct BrainGenerator\n",
    "    number_of_neurons::UnitRange\n",
    "    neuron_type_model::Chain\n",
    "    neuron_type_generators::AbstractArray{NeuronTypeGenerator}\n",
    "end\n",
    "\n",
    "struct Simulation\n",
    "    weights::AbstractArray{Real}\n",
    "    train_loss::Real\n",
    "    test_loss::Real\n",
    "    # data::Ref{AbstractArray}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b137c34",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "\u001b[91mUndefVarError: NeuronTypeGenerator not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: NeuronTypeGenerator not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[5]:9"
     ]
    }
   ],
   "source": [
    "params = Dict([\n",
    "    \"node_space_dims\" => 2,\n",
    "    \"init_number_of_dendrites_range\" => 1:3,\n",
    "    \"init_number_of_axons_range\" => 1:3,\n",
    "    \"prior_size\" => 10,\n",
    "    \"key_size\" => 5,\n",
    "    \"number_of_neuron_types\" => 3\n",
    "])\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "generate(ntg::NeuronTypeGenerator, prior::AbstractArray{Real}) = begin\n",
    "\n",
    "    num_dendrites = rand(ntg.number_of_dendrites)\n",
    "    num_axons = rand(ntg.number_of_axons)\n",
    "\n",
    "    dendrites = [ntg.dendrite_generator_model(prior) for _ in 1:num_dendrites]\n",
    "    axons = [ntg.axon_generator_model(prior) for _ in 1:num_axons]\n",
    "    key = ntg.key_model(prior)\n",
    "\n",
    "    all_weights = [dendrites..., axons..., key...]\n",
    "\n",
    "    dendrites = [Node(d) for d in dendrites]\n",
    "    axons = [Node(a) for a in axons]\n",
    "\n",
    "    return all_weights, Neuron(dendrites, key, axons)\n",
    "end\n",
    "\n",
    "generate(bg::BrainGenerator, prior::AbstractArray{Real}) = begin\n",
    "    num_neurons = rand(bg.number_of_neurons)\n",
    "    return [bg.neuron_type_model(prior) for _ in 1:num_neurons]\n",
    "end\n",
    "\n",
    "# i, nt_gen = sample(enumerate(bg.neuron_type_generators), Weights(neuron_type_dstr))\n",
    "# neuron_type_dstr[i] * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4eab2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "\u001b[91mUndefVarError: Chain not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: Chain not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] (::getfield(Main, Symbol(\"##7#8\")))(::Int64) at .\\none:0",
      " [2] iterate at .\\generator.jl:47 [inlined]",
      " [3] collect(::Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##7#8\"))}) at .\\array.jl:619",
      " [4] top-level scope at In[2]:1"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "ntgs = [\n",
    "    NeuronTypeGenerator(\n",
    "        params[\"init_number_of_dendrites_range\"],\n",
    "        params[\"init_number_of_axons_range\"],\n",
    "        Chain(params[\"prior_size\"], params[\"node_space_dims\"]),\n",
    "        Chain(params[\"prior_size\"], params[\"node_space_dims\"]),\n",
    "        Chain(params[\"prior_size\"], params[\"key_size\"])\n",
    "    )\n",
    "    for _ in 1:params[\"number_of_neuron_types\"]\n",
    "]\n",
    "\n",
    "neuron_type_priors = [rand(params[\"prior_size\"]) for _ in 1:params[\"number_of_neuron_types\"]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for ntg in ntgs\n",
    "\n",
    "\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generate(ntg, prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "415325b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "\u001b[91mMethodError: objects of type Array{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},1} are not callable\u001b[39m\n\u001b[91mUse square brackets [] for indexing an Array.\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: objects of type Array{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},1} are not callable\u001b[39m\n\u001b[91mUse square brackets [] for indexing an Array.\u001b[39m",
      "",
      "Stacktrace:",
      " [1] applychain(::Tuple{Array{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},1}}, ::Array{Float64,1}) at C:\\Users\\noone\\.julia\\packages\\Flux\\NpkMm\\src\\layers\\basic.jl:30 (repeats 2 times)",
      " [2] (::Chain{Tuple{Flux.Recur{Flux.GRUCell{Array{Float32,2},Array{Float32,1}}},Array{Dense{typeof(identity),Array{Float32,2},Array{Float32,1}},1}}})(::Array{Float64,1}) at C:\\Users\\noone\\.julia\\packages\\Flux\\NpkMm\\src\\layers\\basic.jl:32",
      " [3] top-level scope at In[46]:1"
     ]
    }
   ],
   "source": [
    "auto_generator(rand(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0d77a",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db13682a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activate! (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "struct PreSynNode\n",
    "    position::AbstractArray{Real}\n",
    "    # value::AbstractArray{Real}\n",
    "    # hard_value::AbstractArray{Real}\n",
    "end\n",
    "\n",
    "struct PostSynNode\n",
    "    position::AbstractArray{Real}\n",
    "    value::AbstractArray{Real}\n",
    "    # hard_value::AbstractArray{Real}\n",
    "end\n",
    "\n",
    "struct Activation\n",
    "    position::AbstractArray{Real}\n",
    "    energy::Real\n",
    "end\n",
    "\n",
    "mutable struct Space\n",
    "    activations::AbstractArray{Activation}\n",
    "    latent_dims::Integer\n",
    "\n",
    "    exh::Real # exhitation (increase to make pre-syn-neurons more visible to post-syn-neurons)\n",
    "    min_exh::Real # minimum exhitation (fixed)\n",
    "    min_syn_act::Real # minimum required synaptic activation (fixed)\n",
    "end\n",
    "\n",
    "struct InputActivationPattern\n",
    "    key::AbstractArray{Pair{Space, PostSynNode}}\n",
    "    value::AbstractArray{Real}\n",
    "end\n",
    "\n",
    "struct OutputActivationPattern\n",
    "    key::AbstractArray{Real}\n",
    "    value::AbstractArray{Pair{Space, PreSynNode}}\n",
    "end\n",
    "\n",
    "struct Neuron\n",
    "    IAP::InputActivationPattern\n",
    "    OAP::OutputActivationPattern\n",
    "    space::Space\n",
    "end\n",
    "\n",
    "struct Network\n",
    "    input_spaces::AbstractArray{Space}\n",
    "    output_spaces::AbstractArray{Space}\n",
    "    hidden_spaces::AbstractArray{Space}\n",
    "    neurons::AbstractArray{Neuron}\n",
    "end\n",
    "\n",
    "# ------------------------------------------------------\n",
    "\n",
    "mean = (x)->sum(x)/length(x)\n",
    "rbfk(x1::AbstractArray{Real, 1}, x2::AbstractArray{Real, 1}, σ::Real) = ℯ ^ -((mean((x1 .- x2) .^ 2) / σ))\n",
    "activate(psn::PostSynNode, a::Activation, min_exh::Real, exh::Real, min_syn_act::Real) = begin\n",
    "    energy = rbfk(psn.position, a.position, min_exh + exh)\n",
    "    return energy >= min_syn_act ? (energy + a.energy) / 2 : Missing\n",
    "end\n",
    "activate(psn::PostSynNode, s::Space) = begin\n",
    "    activations = collect(skipmissing([activate(psn, a, s.min_exh, s.exh, s.min_syn_act) for a in s.activations]))\n",
    "    return length(activations) > 0 ? activations : [0.]\n",
    "end\n",
    "activate(psn::PreSynNode, energy::Real) = Activation(psn.position, energy)\n",
    "activate(pos::AbstractArray, energy::Real) = Activation(pos, energy)\n",
    "activate(iap::InputActivationPattern) = mean([mean(activate(post_sn, s)) for (s, post_sn) in iap.key])\n",
    "activate(oap::OutputActivationPattern, energy::Real) = [(activate(pre_sn, energy), s) for (s, pre_sn) in oap.value]\n",
    "activate(n::Neuron) = begin\n",
    "    input_mean_act_energy = activate(n.IAP)\n",
    "    local_mean_act_energy = rbfk(n.IAP.value, n.OAP.key, n.space.min_exh + n.space.exh)\n",
    "\n",
    "    out_activation_energy = (input_mean_act_energy + local_mean_act_energy) / 2\n",
    "\n",
    "    return activate(n.OAP, out_activation_energy)\n",
    "end\n",
    "activate!(net::Network, x::AbstractArray, input_activation_energy::Real, simulation_steps::Integer) = begin\n",
    "\n",
    "    for s in net.input_spaces\n",
    "        append!(s.activations, [Activation(x, input_activation_energy)])\n",
    "    end\n",
    "\n",
    "    for i in 1:simulation_steps\n",
    "        for n in net.neurons\n",
    "            for (act, space) in activate(n)\n",
    "                append!(space.activations, [act])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return [s.activations for s in net.output_spaces]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38dacfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample (generic function with 7 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct ProtoPreSynNode\n",
    "    position::Normal\n",
    "end\n",
    "\n",
    "struct ProtoPostSynNode\n",
    "    position::Normal\n",
    "    value::Normal\n",
    "end\n",
    "\n",
    "struct ProtoInputActivationPattern\n",
    "    # keys::AbstractArray{Pair{Space, ProtoPostSynNode}}\n",
    "    number_of_keys::UnitRange\n",
    "\n",
    "    key_proto::ProtoPostSynNode\n",
    "    value::Normal\n",
    "end\n",
    "\n",
    "struct ProtoOutputActivationPattern\n",
    "    number_of_values::UnitRange\n",
    "\n",
    "    value_proto::ProtoPreSynNode\n",
    "    key::Normal\n",
    "    # values::AbstractArray{Pair{Space, ProtoPreSynNode}}\n",
    "end\n",
    "\n",
    "struct ProtoNeuron\n",
    "    PIAP::ProtoInputActivationPattern\n",
    "    POAP::ProtoOutputActivationPattern\n",
    "end\n",
    "\n",
    "struct ProtoSpace\n",
    "    latent_dims::UnitRange\n",
    "end\n",
    "\n",
    "struct ProtoNetwork\n",
    "    number_of_neurons::Integer\n",
    "    number_of_input_spaces::Integer\n",
    "    number_of_hidden_spaces::Integer\n",
    "    number_of_output_spaces::Integer\n",
    "    \n",
    "    neuron_template::ProtoNeuron\n",
    "    space_template::ProtoSpace\n",
    "    \n",
    "    sim_steps::Integer\n",
    "\n",
    "    exh::Real\n",
    "    min_exh::Real\n",
    "    min_syn_act::Real\n",
    "end\n",
    "\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# import Base.sample\n",
    "sample(psn::ProtoPreSynNode, size::Integer) = PreSynNode([rand(psn.position) for _ in 1:size])\n",
    "sample(psn::ProtoPostSynNode, size::Integer) = PostSynNode([rand(psn.position) for _ in 1:size], [rand(psn.value) for _ in 1:size])\n",
    "sample(piap::ProtoInputActivationPattern, is::AbstractArray{Space}, space::Space) = begin\n",
    "    f = (s) -> (s => sample(piap.key_proto, s.latent_dims))\n",
    "    num_keys = rand(piap.number_of_keys)\n",
    "    keys = [f(rand(is)) for _ in 1:num_keys]\n",
    "    value = [rand(piap.value) for _ in 1:space.latent_dims]\n",
    "    return InputActivationPattern(keys, value)\n",
    "end\n",
    "sample(poap::ProtoOutputActivationPattern, os::AbstractArray{Space}, space::Space) = begin\n",
    "    f = (s) -> (s => sample(poap.value_proto, s.latent_dims))\n",
    "    num_values = rand(poap.number_of_values)\n",
    "    values = [f(rand(os)) for _ in 1:num_values]\n",
    "    key = [rand(poap.key) for _ in 1:space.latent_dims]\n",
    "    return OutputActivationPattern(key, values)\n",
    "end\n",
    "sample(pn::ProtoNeuron, space::Space, is::AbstractArray{Space}, os::AbstractArray{Space}) = Neuron(sample(pn.PIAP, is, space), sample(pn.POAP, os, space), space)\n",
    "sample(ps::ProtoSpace, exh::Real, min_exh::Real, min_syn_act::Real) = Space([], rand(ps.latent_dims), exh, min_exh, min_syn_act)\n",
    "sample(n::ProtoNetwork, input_latent_dims::Integer, output_latent_dims::Integer) = begin \n",
    "\n",
    "    input_spaces =  [Space([], input_latent_dims, n.exh, n.min_exh, n.min_syn_act) for _ in 1:n.number_of_input_spaces]\n",
    "    hidden_spaces = [sample(n.space_template, n.exh, n.min_exh, n.min_syn_act) for _ in 1:n.number_of_hidden_spaces]\n",
    "    output_spaces = [Space([], output_latent_dims, n.exh, n.min_exh, n.min_syn_act) for _ in 1:n.number_of_output_spaces]\n",
    "\n",
    "    spaces = [input_spaces..., hidden_spaces..., output_spaces...]\n",
    "    \n",
    "    neurons = [[sample(n.neuron_template, \n",
    "                    spaces[n.number_of_input_spaces + i], \n",
    "                    spaces[1:(n.number_of_input_spaces + i - 1)],\n",
    "                    spaces[(n.number_of_input_spaces + i + 1):end]) for _ in 1:Integer(floor(n.number_of_neurons / n.number_of_hidden_spaces))] for i in 1:n.number_of_hidden_spaces]\n",
    "    neurons = vcat(neurons...)\n",
    "\n",
    "    return Network(input_spaces, output_spaces, hidden_spaces, neurons)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e812812",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_samplen_proto_neuron(num_IAP_range, num_OAP_range; μ=0, σ=1) = begin\n",
    "\n",
    "    base_post_sn = ProtoPostSynNode(Normal(μ, σ), Normal(μ, σ))\n",
    "    base_pre_sn = ProtoPreSynNode(Normal(μ, σ))\n",
    "\n",
    "    base_iap = ProtoInputActivationPattern(num_IAP_range, base_post_sn, Normal(μ, σ))\n",
    "    base_oap = ProtoOutputActivationPattern(num_OAP_range, base_pre_sn, Normal(μ, σ))\n",
    "\n",
    "    return ProtoNeuron(base_iap, base_oap)\n",
    "end\n",
    "gen_samplen_proto_network(params::Dict) = begin\n",
    "    base_neuron = gen_samplen_proto_neuron(params[\"num_IAP_range\"], params[\"num_OAP_range\"], μ=params[\"start_μ\"], σ=params[\"start_σ\"])\n",
    "    base_space = ProtoSpace(params[\"hidden_latent_dims_range\"])\n",
    "\n",
    "    return ProtoNetwork(params[\"num_neurons\"], params[\"num_input_spaces\"], params[\"num_hidden_spaces\"], params[\"num_output_spaces\"], base_neuron, base_space, params[\"sim_steps\"], params[\"start_exh\"], params[\"min_exh\"], params[\"min_syn_act\"])\n",
    "end\n",
    "\n",
    "\n",
    "network_params = Dict([\n",
    "    \"min_exh\" => 0.01,\n",
    "    \"start_exh\" => 4,\n",
    "    \"min_syn_act\" => 0.01,\n",
    "    \"sim_steps\" => 3,\n",
    "    \"hidden_latent_dims_range\" => 3:10,\n",
    "    \"num_IAP_range\" => 1:3,\n",
    "    \"num_OAP_range\" => 1:3,\n",
    "    \"num_input_spaces\" => 1,\n",
    "    \"num_hidden_spaces\" => 10,\n",
    "    \"num_output_spaces\" => 1,\n",
    "    \"num_neurons\" => 10,\n",
    "    \"start_μ\" => 0,\n",
    "    \"start_σ\" => 1,\n",
    "    \"input_activation_energy\" => 1\n",
    "]);\n",
    "\n",
    "training_params = Dict([\n",
    "    \"input_latent_dims\" => 400,\n",
    "    \"output_latent_dims\" => 400,\n",
    "    \"simulation_steps\" => 1\n",
    "]);\n",
    "\n",
    "global_params = Dict([\n",
    "    \"max_utf\" => 400\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e84d14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_netork = gen_samplen_proto_network(network_params)\n",
    "network_00 = sample(proto_netork, training_params[\"input_latent_dims\"], training_params[\"output_latent_dims\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e7128df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Array{Activation,1},1}:\n",
       " [Activation(Real[1.49748, -0.0406526, -0.15701, 0.436242, 0.279698, -0.156771, 0.384979, 0.276601, 0.0353812, 1.59884  …  -0.193803, -1.47102, -0.329973, 0.875728, -1.4175, -2.93716, 1.1633, 0.997665, -0.573168, 0.488198], 0.5346), Activation(Real[-0.0989419, 0.185197, -0.088238, -0.330577, 0.565095, 0.439265, -0.732064, -1.55358, -0.3798, 0.712892  …  -2.84026, -0.673988, -1.66995, -0.627776, 0.607456, -0.240589, 0.121678, 1.18012, -0.500065, 0.758764], 0.296408), Activation(Real[0.275755, -0.231182, -1.28144, -0.755422, -0.299208, 0.466606, -0.143905, -0.341089, 0.550356, -0.807807  …  -0.155673, -0.940883, 0.861228, 2.35039, -0.216059, -0.671927, 0.840653, 0.0329151, -0.0213165, -0.389263], 0.535804), Activation(Real[-0.886827, -0.812949, 1.78077, 0.133835, 0.342387, 0.577872, 0.807386, 0.561151, -0.696364, -1.04517  …  -0.891114, -0.249687, -1.43091, -1.42037, -1.12378, -0.5253, -0.207816, -1.67737, -1.028, 0.644099], 0.317522), Activation(Real[-0.344658, 1.0423, 1.02894, 0.17318, -0.262941, -1.67494, -0.829922, 1.2361, 0.292384, 1.88318  …  -0.00537936, -0.728198, 0.102345, -1.59586, -1.55765, 0.519483, 1.07328, -0.227461, 0.920529, 0.805723], 0.585814), Activation(Real[2.09068, 1.7196, 2.51091, 0.632858, -0.550461, 0.954333, 0.791764, -0.686829, 0.582648, -0.840959  …  -0.975183, -1.91703, -0.157877, 0.317489, -0.831576, 1.03306, 0.436132, -0.0641086, 0.361228, 0.365009], 0.48074), Activation(Real[0.279912, 1.45347, 0.951065, -0.732722, 0.691247, -0.289367, 0.87435, 1.97003, -0.706514, 0.249817  …  0.140452, 0.352867, 0.3384, -1.31258, -0.278094, 1.41437, -1.85209, 0.174881, 1.06496, 0.454782], 0.48074)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_int(s::String, max_utf::Int; placeholder = 0) = map(x -> Int(x) < max_utf ? Int(x) : placeholder, Vector{Char}(s))\n",
    "text_to_onehot(x::String, max_utf=400) = vcat([vcat([ci != i ? 0. : 1. for i in 1:max_utf]) for ci in enumerate(string_to_int(x, max_utf))])\n",
    "n_gram(x::AbstractArray, n::Int) = [x[i:i+n-1] for i in 1:length(x)-n+1]\n",
    "kernel_select(x::AbstractArray, r::Int) = [x[max(1, abs(i-r)):min(length(x), i+r)] for i in 1:length(x)]\n",
    "\n",
    "x = text_to_onehot(\"hi this is jonny\", global_params[\"max_utf\"])\n",
    "\n",
    "activate!(network_00, x[1], training_params[\"simulation_steps\"], network_params[\"input_activation_energy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0512fa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "params (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flux.params(network_00.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b443dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Array{T,1} where T,1}:\n",
       " Real[]                                                                                                                  \n",
       " Real[]                                                                                                                  \n",
       " Real[]                                                                                                                  \n",
       " [0.463739, 0.463739]                                                                                                    \n",
       " [0.610508, 0.45736, 0.610508, 0.45736]                                                                                  \n",
       " [0.463739, 0.603225, 0.463739, 0.603225]                                                                                \n",
       " [0.610508, 0.45736, 0.610508, 0.45736]                                                                                  \n",
       " [0.610508, 0.415826, 0.548126, 0.610508, 0.415826, 0.548126]                                                            \n",
       " [0.715228, 0.463739, 0.415826, 0.548126, 0.548126, 0.490167, 0.715228, 0.463739, 0.415826, 0.548126, 0.548126, 0.490167]\n",
       " [0.715228, 0.490167, 0.69598, 0.715228, 0.490167, 0.69598]                                                              "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[ss.energy for ss in s.activations] for s in network_00.hidden_spaces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "julian-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_segmentation(x::String; term_char = ' ') = string.(split(x, ' '), term_char)\n",
    "\n",
    "# unit_cbit(x::Number, max_x::Number) = exp(-2*pi*im*(x/(max_x)))\n",
    "\n",
    "# text_to_cbits(x::String; max_utf=400) = [unit_cbit.(v, max_utf) for v in str_to_int(x, max_utf)]\n",
    "\n",
    "# unit_phase(a::Complex) = angle(a) / pi;\n",
    "\n",
    "# neighbour_smooth(x::AbstractArray{Complex{Float64}, 1}, r::Int) = [mean(k) for k in kernel_select(x, r)]\n",
    "\n",
    "# cbit_rotation_encode(x::Complex, r::Complex, exp_len::Int, t::Int32, Δt) = Δt * (r * x)\n",
    "# cbit_rotation_encode(x::AbstractArray{Complex{Float64}, 1}, r::AbstractArray{Complex{Float64}, 1}, exp_len::Int) = sqrt.((r * x') .^ exp_len)\n",
    "\n",
    "# amplitude(a::Complex) = sqrt(real(a)^2 + imag(a)^2);\n",
    "# unit_phase(a::Complex) = angle(a) / pi;\n",
    "# quantify(A::Real, phy::Real) = A * exp(phy * pi * im);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058278ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ce7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Distributions.Normal\n",
    "\n",
    "# struct AlphaNode\n",
    "#     space_range::UnitRange{Integer}\n",
    "#     destination_space_range::UnitRange{Integer}\n",
    "#     location_normal::Normal\n",
    "#     value_normal::Normal\n",
    "#     input_size_range::UnitRange{Integer}\n",
    "#     output_size_range::UnitRange{Integer}\n",
    "# end\n",
    "\n",
    "# struct Node\n",
    "#     location::AbstractArray{Real}\n",
    "\n",
    "#     soft_value::AbstractArray{Real}\n",
    "#     hard_value::AbstractArray{Real}\n",
    "# end\n",
    "\n",
    "# struct NodeSpace\n",
    "\n",
    "#     all_nodes::AbstractArray{Node}\n",
    "\n",
    "#     ex::Real     # controlled, correlates with influence complexity\n",
    "#     ex_min::Real # static\n",
    "# end\n",
    "\n",
    "# struct Connectum\n",
    "#     nodes::AbstractArray{Node}\n",
    "#     source_space::NodeSpace\n",
    "#     destination_space::NodeSpace\n",
    "# end\n",
    "\n",
    "# struct Network\n",
    "#     entry_spaces::AbstractArray{NodeSpace}\n",
    "# end\n",
    "\n",
    "# struct Activation\n",
    "#     node_space::NodeSpace\n",
    "#     value::AbstractArray{Real}\n",
    "#     strength::Real\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9d44fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91msyntax: unexpected \")\"\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91msyntax: unexpected \")\"\u001b[39m",
      ""
     ]
    }
   ],
   "source": [
    "# global_ex_min = 0.01\n",
    "# global_ex_init = 10\n",
    "\n",
    "\n",
    "# import Base.sample\n",
    "# function Base.sample(an::AlphaNode) \n",
    "#     Node(sample(an.space_range), sample(an.destination_space_range), sample(an.location_normal, sample(an.input_size_range)), sample(an.value_normal, sample(an.output_size_range)))\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "# function activate(net::Network, x::AbstractArray{AbstractArray{Real}})\n",
    "\n",
    "#     activations_0 = [activate(es, x) for es in net.entry_spaces]\n",
    "\n",
    "    \n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "# function activate(n::Node, x::AbstractArray{Real}) \n",
    "#     activation_energy = e .^ -((norm(x .- n.location) / (ns.ex_min + ns.ex)))\n",
    "#     return activation_energy == 0 ? Missing : activation_energy\n",
    "# end\n",
    "\n",
    "# function activate(c::Connectum, activations::AbstractArray{Activation}, x::AbstractArray[Real])\n",
    "\n",
    "#     energy = c.nodes\n",
    "\n",
    "#     Activation(ns, n.value, activation_energy)\n",
    "# end\n",
    "\n",
    "# function propergate(act::Activation)\n",
    "#     [activate(n, act.node_space, for n in act.node_space.nodes]\n",
    "# end\n",
    "\n",
    "# # activate(d::Dendrite, x::AbstractArray{Real}) = d.value * activate(d.key, x)\n",
    "# # activate(a::Axon, x::AbstractArray{Real}) = a.value * activate(a.key, x)\n",
    "# # activate(n::Neuron, x::AbstractArray{Real}) = sum([activate(n.axon, activate(d, x)) for d in n.dendrites]) / length(n.dendrites)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "seven-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all agent recieve the same truths and the same assumptions but cannot change them\n",
    "# all agents can change the contradictions and falses \n",
    "# any agent can take a step in the environment\n",
    "#   (1) they are presented with another set of truths, contradictions and falses\n",
    "#   (2) they choose which to keep and which to discard. (the weighting is an indicator for \"reason why kept/discarded\")\n",
    "#   (3) they produce and omit a false/contradiciton feeding it to the other agents (scored by the number of agents accepting it)\n",
    "#   (repeat untill maximum number of truths accumulated)\n",
    "# now the prediction about the assumptions (always as contradictions) gives a maximum likeliehood estimate.\n",
    "# optimally no falses and contradictions remain after the process has finished.\n",
    "\n",
    "# causal chain prediction\n",
    "# (1) sample n events in temporal order (closer events beeing more likely)\n",
    "# (2) have an agent supplement these events with generated ones trying to persuade a false prediction\n",
    "# (3) have another agent try to remove false events\n",
    "# (4) that same agent also predicts the chose lable\n",
    "\n",
    "# py\"\"\"\n",
    "# import subprocess\n",
    "# subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"wikipedia\"])\n",
    "# import wikipedia\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-production",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "residential-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when \"awake\"\n",
    "# a group of quanti encodings on a ring are querried when its group_uuid quanti is closest\n",
    "# a group of quanti encodings on a ring is created when a no group_uuid quanti is close enough\n",
    "# new quanti in such a ring are established if no quanti is close enough\n",
    "# quanti, that are querried, hold:\n",
    "#     (1) the current activation (intrinsically affected by decay)\n",
    "#     (2) the total activation (is important for \"fitness\" when sleeping)\n",
    "# the group itself hold:\n",
    "#     (1) the correlation activation (\"correlated fitness\")\n",
    "\n",
    "# when \"sleeping\"\n",
    "# a group of quanti encodings relaxes onto a ring, accounting for:\n",
    "#    (1) their \"fitness\" weights the quanti's difusion's\n",
    "#    (2) \"correlated fitness\" makes two quanti attract\n",
    "# there is an equilibrium where \"fitness\" is just large enough so that \"correlated fitness\" cannot cross the barrier to merge the two quanti\n",
    "\n",
    "\n",
    "# to take the example from above, the blue quanti represent the optimal configuration if all possible quanties stored here occure uniformly\n",
    "\n",
    "# the goal is to have a \"normal\" text (such as the example) occupy a high entropy state like the blue quanties \n",
    "# the goal is also to have rings store information about some input domain specified in a tree format\n",
    "\n",
    "\n",
    "# on the output side i'm thinking of fetchng each quanti that was activated and their position as one large \"encoding\"\n",
    "\n",
    "\n",
    "# given that N x N comparisons are the most important, distilation and representation learning is the most important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
